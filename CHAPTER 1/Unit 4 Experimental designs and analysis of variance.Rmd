---
title: "Theories of Biostatistics"
subtitle: "Unit 4: Experimental designs and analysis of variance"
author: "Semakula Muhammed, PhD - Senior Statistician"
date: "`O3 November 2025`"
output:
  ioslides_presentation:
    widescreen: true
    smaller: false
    transition: default
    logo: NULL
  beamer_presentation:
    theme: "Madrid"
    colortheme: "default"
    fonttheme: "structurebold"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, 
                      fig.width = 6, fig.height = 4)
library(ggplot2)
library(dplyr)
```

# Introduction

## Strategy of Experimentation

**What is Experimental Design?**

- Systematic approach to investigation
- Planning how to gather information efficiently
- Making objective conclusions from data
- Improving products and processes

**Key Question:** How do we learn about systems and processes effectively?

## Applications of Experimental Design

**Industrial Applications:**

- Product/process development and improvement
- Quality improvement and defect reduction
- Performance optimization
- Robustness studies

**Research Applications:**

- Scientific research (chemistry, biology, physics)
- Engineering research
- Medical and clinical trials
- Agricultural studies

## Basic Principles of Experimental Design

**Three Fundamental Principles:**

1. **Randomization**
   - Reduces bias
   - Ensures validity of statistical tests
   
2. **Replication**
   - Provides estimate of experimental error
   - Increases precision
   
3. **Blocking**
   - Controls nuisance factors
   - Reduces experimental error

## Guidelines for Designing Experiments

**Seven-Step Process:**

1. Recognition and statement of the problem
2. Selection of response variable(s)
3. Choice of factors, levels, and ranges
4. Choice of experimental design
5. Performing the experiment
6. Statistical analysis of data
7. Conclusions and recommendations

## Recognition of Problem

- Clear objective statement
- Understanding the problem thoroughly
- Identifying practical questions to answer
- Considering all stakeholders

**Example Questions:**

- What factors affect the response?
- Which factors are most important?
- What settings optimize the response?

## Response Variables and Factors

**Response Variable:**

- Measured output (dependent variable)
- Should be directly related to objectives
- Consider measurement capability

**Factors:**

- Design factors (we can control)
- Nuisance factors (affect response but not of interest)
- Levels: values at which factors are tested

---

# Simple Comparative Experiments

## Introduction to Comparative Experiments

**Basic Problem:**

- Comparing two conditions/treatments
- Example: New vs. old process
- Need statistical methods for valid conclusions

**Two Main Designs:**

1. Completely Randomized Design
2. Paired Comparison Design

## Statistical Concepts Review

**Population vs. Sample:**

- Population: entire group of interest
- Sample: subset selected for study

**Parameters vs. Statistics:**

- Parameters: population characteristics (μ, σ²)
- Statistics: sample characteristics ($\bar{x}$, s²)

**Goal:** Make inferences about population from sample

## Hypothesis Testing Framework

**Two Hypotheses:**

- **Null Hypothesis (H₀):** No difference between treatments
- **Alternative Hypothesis (H₁):** There is a difference

**Decision Making:**

- Type I Error (α): Reject H₀ when true
- Type II Error (β): Fail to reject H₀ when false
- Power = 1 - β

## Two-Sample t-Test

**Comparing Two Means:**

When σ₁² = σ₂² = σ² (pooled variance):

$$t_0 = \frac{\bar{x}_1 - \bar{x}_2}{s_p\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}}$$

where $s_p^2 = \frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1 + n_2 - 2}$

**Degrees of freedom:** n₁ + n₂ - 2

## Confidence Intervals

**For the Difference in Means:**

$$(\bar{x}_1 - \bar{x}_2) \pm t_{\alpha/2, n_1+n_2-2} \cdot s_p\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}$$

**Interpretation:**

- We are (1-α)×100% confident the true difference lies in this interval
- If interval includes 0, no significant difference

## Sample Size Determination

**Key Question:** How many observations needed?

**Formula depends on:**

- Desired significance level (α)
- Desired power (1-β)
- Effect size (difference to detect)
- Variance estimate

**Operating Characteristic (OC) Curves** help determine sample size

## Paired Comparison Design

**When to Use:**

- Each experimental unit receives both treatments
- Before-after comparisons
- Matched pairs of units

**Advantage:**

- Eliminates variability between units
- More powerful than completely randomized design
- Requires fewer observations

## Paired t-Test

**Test Statistic:**

$$t_0 = \frac{\bar{d}}{s_d / \sqrt{n}}$$

where:

- d = difference within each pair
- $\bar{d}$ = mean difference
- s_d = standard deviation of differences
- n = number of pairs

**Degrees of freedom:** n - 1

---

# Single-Factor ANOVA

## Analysis of Variance (ANOVA)

**Purpose:**

- Compare more than two treatment means
- Extension of t-test to multiple groups
- Tests H₀: μ₁ = μ₂ = ... = μₐ

**Key Idea:**

- Partition total variability into components
- Compare between-treatment vs. within-treatment variation

## ANOVA Model

**Fixed Effects Model:**

$$y_{ij} = \mu + \tau_i + \epsilon_{ij}$$

where:

- yᵢⱼ = jth observation in ith treatment
- μ = overall mean
- τᵢ = ith treatment effect
- εᵢⱼ = random error ~ N(0, σ²)

**Assumptions:**

- Independence, Normality, Equal variance

## Decomposition of Total Variability

**Sum of Squares Identity:**

$$SS_T = SS_{Treatments} + SS_E$$

where:

- $SS_T = \sum\sum (y_{ij} - \bar{y}_{..})^2$ (Total)
- $SS_{Treatments} = n\sum (\bar{y}_{i.} - \bar{y}_{..})^2$ (Between)
- $SS_E = \sum\sum (y_{ij} - \bar{y}_{i.})^2$ (Within/Error)

## ANOVA Table

| Source | SS | df | MS | F₀ |
|--------|----|----|----|----|
| Treatments | SS_Treatments | a-1 | MS_Treatments | MS_Treatments/MS_E |
| Error | SS_E | a(n-1) | MS_E | |
| Total | SS_T | an-1 | | |

**Decision Rule:**

Reject H₀ if F₀ > F_{α, a-1, a(n-1)}

## Model Adequacy Checking

**Four Key Diagnostic Plots:**

1. **Normal probability plot** of residuals
   - Check normality assumption
   
2. **Residuals vs. fitted values**
   - Check equal variance
   
3. **Residuals vs. order**
   - Check independence
   
4. **Residuals vs. other variables**
   - Identify lurking variables

## Multiple Comparisons

**Fisher's LSD Method:**

$$LSD = t_{\alpha/2, a(n-1)} \sqrt{2MS_E/n}$$

**Tukey's HSD Method:**

$$T_{\alpha} = q_{\alpha}(a, f) \sqrt{MS_E/n}$$

**When to Use:**

- Use after significant F-test
- Controls error rates differently

## Contrasts

**Definition:** Linear combination of treatment means

$$\Gamma = \sum_{i=1}^a c_i \mu_i$$

where $\sum c_i = 0$

**Examples:**

- Compare one treatment to average of others
- Compare groups of treatments
- Polynomial trends

## Orthogonal Contrasts

**Properties:**

- Contrasts are independent
- Partition SS_Treatments into single df components
- Maximum of (a-1) orthogonal contrasts

**Orthogonality Condition:**

$$\sum_{i=1}^a c_{li}c_{mi} = 0$$

## Random Effects Model

**Model:**

$$y_{ij} = \mu + \tau_i + \epsilon_{ij}$$

where τᵢ ~ N(0, σ²_τ)

**Difference from Fixed Effects:**

- Treatments are random sample from population
- Interest in variance components
- Different expected mean squares

## Variance Component Estimation

**Method of Moments:**

$$\hat{\sigma}^2_\tau = \frac{MS_{Treatments} - MS_E}{n}$$

**Interpretation:**

- Measures variability between treatment levels
- Used in reliability, quality control
- Important for hierarchical designs

---

# Randomized Blocks & Latin Squares

## Randomized Complete Block Design (RCBD)

**Purpose:**

- Control one nuisance factor
- Increase precision by blocking
- Reduce experimental error

**Structure:**

- Treatments randomly assigned within each block
- Each treatment appears once per block

## RCBD Model

**Statistical Model:**

$$y_{ij} = \mu + \tau_i + \beta_j + \epsilon_{ij}$$

where:

- τᵢ = treatment effect
- βⱼ = block effect
- εᵢⱼ ~ N(0, σ²)

## RCBD ANOVA Table

| Source | SS | df | MS | F₀ |
|--------|----|----|----|----|
| Treatments | SS_Treatments | a-1 | MS_Treatments | MS_Treatments/MS_E |
| Blocks | SS_Blocks | b-1 | MS_Blocks | MS_Blocks/MS_E |
| Error | SS_E | (a-1)(b-1) | MS_E | |
| Total | SS_T | ab-1 | | |

**Note:** Only test treatments, blocks are nuisance

## Efficiency of RCBD

**Relative Efficiency:**

$$RE = \frac{MS_{Blocks} + (b-1)MS_E}{b \cdot MS_E}$$

**Interpretation:**

- RE > 1: RCBD more efficient than CRD
- Larger RE = greater benefit from blocking
- Measures reduction in sample size needed

## Latin Square Design

**Purpose:**

- Control TWO nuisance factors simultaneously
- Very efficient design
- Requires a² observations for 'a' treatments

**Structure:**

- Each treatment appears once in each row
- Each treatment appears once in each column

## Latin Square Model

**Statistical Model:**

$$y_{ijk} = \mu + \alpha_i + \tau_j + \beta_k + \epsilon_{ijk}$$

where:

- αᵢ = row effect
- τⱼ = treatment effect  
- βₖ = column effect
- Only (a-1)(a-2) df for error

## Latin Square ANOVA

| Source | df |
|--------|-----|
| Rows | a-1 |
| Columns | a-1 |
| Treatments | a-1 |
| Error | (a-1)(a-2) |
| Total | a²-1 |

**Limitation:** Small error df for small designs

## Graeco-Latin Square

**Purpose:**

- Control THREE nuisance factors
- Superimpose two Latin squares
- Uses Greek and Latin letters

**Example (4×4):**

| | | | |
|---|---|---|---|
| Aα | Bβ | Cγ | Dδ |
| Bγ | Aδ | Dα | Cβ |
| Cδ | Dγ | Bβ | Aα |
| Dβ | Cα | Aδ | Bγ |

## Balanced Incomplete Block Design (BIBD)

**When to Use:**

- Cannot fit all treatments in one block
- Block size (k) < number of treatments (a)

**Parameters:**

- a = number of treatments
- b = number of blocks
- k = block size
- r = replications per treatment
- λ = number of times each pair appears together

## BIBD Properties

**Balance Conditions:**

- ak = br (total observations)
- λ(a-1) = r(k-1)

**Advantages:**

- All pairwise comparisons equally precise
- Efficient for large number of treatments
- Flexible design

---

# Introduction to Factorials

## What is a Factorial Design?

**Definition:**

- Investigate effects of 2+ factors simultaneously
- Each factor at 2+ levels
- All combinations tested

**Notation:**

- Factors: A, B, C, ...
- Levels: a, b, c, ...
- Full factorial: a × b × c × ... treatment combinations

## Advantages of Factorial Designs

**1. Efficiency:**

- More information per observation
- Study multiple factors with same resources

**2. Interaction Detection:**

- Identify when factor effects depend on other factors
- Cannot detect with one-factor-at-a-time

**3. Broader Inference:**

- Conclusions valid over wider range of conditions

## Main Effects

**Definition:** Average effect of changing a factor level

**Factor A Main Effect:**

$$A = \bar{y}_{high} - \bar{y}_{low}$$

**Interpretation:**

- Positive: Response increases with A
- Negative: Response decreases with A
- Near zero: No effect

## Interaction Effects

**Definition:** When effect of one factor depends on level of another

**AB Interaction:**

$$AB = \frac{1}{2}[(y_{high,high} - y_{low,high}) - (y_{high,low} - y_{low,low})]$$

**Graphical Check:**

- Parallel lines = No interaction
- Non-parallel lines = Interaction present

## Two-Factor Factorial Model

**Fixed Effects Model:**

$$y_{ijk} = \mu + \tau_i + \beta_j + (\tau\beta)_{ij} + \epsilon_{ijk}$$

where:

- τᵢ = effect of ith level of A
- βⱼ = effect of jth level of B
- (τβ)ᵢⱼ = interaction effect
- εᵢⱼₖ ~ N(0, σ²)

## Two-Factor ANOVA Table

| Source | df | MS | F₀ |
|--------|----|----|-----|
| A | a-1 | MS_A | MS_A/MS_E |
| B | b-1 | MS_B | MS_B/MS_E |
| AB | (a-1)(b-1) | MS_AB | MS_AB/MS_E |
| Error | ab(n-1) | MS_E | |
| Total | abn-1 | | |

**Test hierarchy:** Test interaction first!

## Interpreting Results

**If interaction is significant:**

- Main effects may be misleading
- Focus on simple effects at each level
- Use interaction plots

**If interaction is not significant:**

- Interpret main effects directly
- Simpler conclusions

## General Factorial Design

**Multiple Factors:**

$$y = \mu + \text{main effects} + \text{2-way interactions} + \text{3-way interactions} + ... + \epsilon$$

**Complications:**

- Number of terms increases rapidly
- Higher-order interactions often negligible
- May need fractional designs

## Response Surfaces

**Purpose:**

- Visualize factor effects
- Find optimal conditions
- Understand response behavior

**Methods:**

- Contour plots
- 3D surface plots
- Mathematical models

## Blocking in Factorial Designs

**Key Idea:**

- Arrange factorial runs in blocks
- Control nuisance factors
- May need to confound some effects

**Simple Approach:**

- Replicate complete factorial in each block
- Block becomes another factor in ANOVA

---

# The 2^k Factorial Design

## Introduction to 2^k Designs

**Characteristics:**

- k factors, each at 2 levels (low/high)
- 2^k treatment combinations
- Very efficient designs
- Foundation for fractional factorials

**Notation:**

- Factors: A, B, C, ...
- Low level: -1 (or -)
- High level: +1 (or +)

## The 2² Design

**Four Treatment Combinations:**

1. (1): both factors at low
2. a: A high, B low
3. b: A low, B high  
4. ab: both factors at high

**Effects Estimation:**

- A effect: [a + ab - (1) - b] / 2n
- B effect: [b + ab - (1) - a] / 2n
- AB interaction: [ab + (1) - a - b] / 2n

## Geometric View of 2² Design

```{r, echo=FALSE, fig.width=5, fig.height=4}
# Create a 2^2 design visualization
library(ggplot2)
design_22 <- data.frame(
  A = c(-1, 1, -1, 1),
  B = c(-1, -1, 1, 1),
  label = c("(1)", "a", "b", "ab")
)

ggplot(design_22, aes(x = A, y = B)) +
  geom_point(size = 4, color = "blue") +
  geom_text(aes(label = label), vjust = -1, size = 5) +
  geom_hline(yintercept = 0, linetype = "dashed", alpha = 0.3) +
  geom_vline(xintercept = 0, linetype = "dashed", alpha = 0.3) +
  xlim(-1.5, 1.5) + ylim(-1.5, 1.5) +
  labs(title = "2² Factorial Design", 
       x = "Factor A", y = "Factor B") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))
```

## The 2³ Design

**Eight Treatment Combinations:**

(1), a, b, ab, c, ac, bc, abc

**Seven Effects:**

- 3 main effects: A, B, C
- 3 two-factor interactions: AB, AC, BC
- 1 three-factor interaction: ABC

## Effect Calculation Algorithm

**Sign Table Method:**

1. Create design matrix with ± signs
2. Multiply columns for interactions
3. Sum products with response
4. Divide by n2^(k-1)

**Regression Approach:**

- Code factors as ±1
- Fit regression model
- Coefficients = effects/2

## General 2^k Design

**Number of Effects:**

Total effects = 2^k - 1

**Breakdown:**

- Main effects: k
- 2-factor interactions: C(k,2)
- 3-factor interactions: C(k,3)
- ...
- k-factor interaction: 1

## Single Replicate 2^k Design

**Challenge:**

- No replication, no pure error estimate
- Need alternative for testing significance

**Solutions:**

1. Normal probability plot of effects
2. Lenth's method
3. Pool high-order interactions as error

## Normal Probability Plot

**Principle:**

- Effects from inactive factors ~ N(0, σ²)
- Plot effects on normal probability paper
- Significant effects fall off line

**Interpretation:**

- Points on line: negligible effects
- Points off line: significant effects

## Lenth's Method

**Pseudo Standard Error (PSE):**

$$PSE = 1.5 \times \text{median}|E_i|$$

where Eᵢ are effect estimates

**Margin of Error:**

$$ME = t_{0.975, m} \times PSE$$

**Simultaneous Margin of Error (SME)** for multiple comparisons

## Analysis Strategy for Unreplicated 2^k

**Step-by-step:**

1. Calculate all effects
2. Create normal probability plot
3. Identify significant effects
4. Fit reduced model with significant terms
5. Analyze residuals
6. Interpret results

## Center Points

**Purpose:**

- Estimate curvature (quadratic effects)
- Check model adequacy
- Estimate pure error

**Test for Curvature:**

$$F = \frac{n_F n_C (\bar{y}_F - \bar{y}_C)^2}{(n_F + n_C)MS_E}$$

where F = factorial points, C = center points

## Why Code Variables?

**Benefits:**

1. Simplifies calculations
2. Standardizes scale
3. Makes regression coefficients comparable
4. Improves numerical stability

**Standard Coding:**

$$x = \frac{\text{natural units} - \text{mean}}{\text{half range}}$$

---

# Blocking and Confounding

## Why Block Factorial Designs?

**Challenges:**

- Large factorials require many runs
- Cannot complete in one homogeneous batch
- Time, material, or equipment constraints

**Solution:**

- Divide runs into blocks
- Sacrifice some information (confound effects with blocks)
- Strategic confounding of least important effects

## Blocking a Replicated 2^k

**Simple Case:**

- 2^k design with n replicates
- Create n blocks of size 2^k
- Each block is complete replicate
- No confounding

**ANOVA includes block factor**

## Confounding in 2^k Designs

**Principle:**

- Deliberately confound high-order interactions with blocks
- Assume these interactions negligible
- Trade information for feasibility

**Key:** Choose effects to confound carefully

## Confounding 2^k in Two Blocks

**Procedure:**

1. Choose effect to confound (usually highest interaction)
2. Divide runs into two groups based on signs
3. One group per block

**Example (2³):**

Confound ABC with blocks

- Block 1: (1), ab, ac, bc (minus signs)
- Block 2: a, b, c, abc (plus signs)

## Block Defining Contrast

**Definition:** Effect confounded with blocks

**Notation:** L = defining contrast

**Properties:**

- L = 0 → Block 1
- L = 1 → Block 2
- Cannot estimate L independently

## Importance of Blocking

**Benefits:**

- Reduces experimental error
- Controls systematic variation
- Increases precision of remaining estimates

**Cost:**

- Loss of information on confounded effects
- Additional complexity in design

## Confounding in Four Blocks

**Need:** Confound TWO effects with blocks

**2² = 4 blocks:**

- Choose two defining contrasts: L₁, L₂
- Generalized interaction: L₁L₂
- Three effects confounded total

**Example (2⁴):**

Confound ABD and ACD with blocks

## Confounding in 2^p Blocks

**General Case:**

- p defining contrasts needed
- 2^p - 1 effects confounded total
- Choose p independent effects
- Remaining confounded = generalized interactions

## Partial Confounding

**Idea:**

- Use different confounding schemes in different replicates
- Recover information on all effects
- Combine information from multiple replicates

**Advantage:** No effect completely confounded

---

# Fractional Factorial Designs

## Why Fractional Factorials?

**Motivation:**

- Full 2^k too large for screening
- Many interactions likely negligible
- Example: 2⁷ = 128 runs

**Solution:**

- Run fraction of full factorial
- 2^(k-p) design (1/2^p fraction)
- Strategic selection of runs

## Sparsity of Effects Principle

**Key Assumption:**

- Most systems dominated by main effects
- Some two-factor interactions
- Higher-order interactions negligible

**Implication:**

- Don't need all 2^k runs
- Fraction provides sufficient information

## One-Half Fraction: 2^(k-1) Design

**Concept:**

- Run half of full factorial (2^(k-1) runs)
- Choose runs using generator
- Effects are aliased in pairs

**Generator:** I = ±(effect)

**Example (2^(4-1)):**

Generator: I = ABCD

## Aliasing Structure

**Definition:** Two effects estimated by same contrast

**Alias Relationships:**

If I = ABCD (2^(4-1) design):

- A aliased with BCD
- B aliased with ACD
- AB aliased with CD

**Notation:** A = BCD (read "A is aliased with BCD")

## Design Resolution

**Definition:** Length of shortest word in defining relation

**Classifications:**

- **Resolution III:** Main effects aliased with 2-factor interactions
- **Resolution IV:** Main effects clear of 2-factor interactions
- **Resolution V:** Main effects and 2-factor interactions clear

**Notation:** 2^(k-p)_R (e.g., 2^(7-4)_III)

## Construction of Half Fractions

**Steps:**

1. Write full factorial for k-1 factors
2. Choose generator (highest interaction as factor k)
3. Add kth factor using signs from generator
4. Determine alias structure

**Principal Fraction:** I = +generator

## One-Quarter Fraction: 2^(k-2)

**Two generators needed:**

- Choose two independent effects
- Third effect aliased by generalized interaction
- Four alias chains

**Example (2^(6-2)):**

Generators: I = ABCE, I = BCDF

Generalized: I = ADEF

## General 2^(k-p) Designs

**p generators required:**

- 2^p - 1 defining contrasts
- 2^p - 1 aliases per effect
- Choose generators for best resolution

**Minimum Aberration:** Design with least aliasing

## Analyzing Fractional Factorials

**Steps:**

1. Estimate contrasts from data
2. Identify alias structure
3. Assume negligible effects
4. Interpret remaining effects
5. Confirm with residual analysis

**Key:** Clear interpretation requires resolution ≥ IV

## Fold-Over Designs

**Purpose:**

- De-alias specific effects
- Convert resolution III → IV
- Targeted follow-up

**Single Factor Fold-Over:**

- Reverse signs of one factor
- De-aliases that factor's effects

**Full Fold-Over:**

- Reverse all signs
- Doubles sample size
- De-aliases all effects

## Plackett-Burman Designs

**Characteristics:**

- N = 4k runs (k factors)
- Not from 2^k series
- Resolution III
- Complex alias structure

**Use:** Screening many factors efficiently

## Resolution IV Designs

**Properties:**

- Main effects clear of 2FI
- 2FI aliased with each other
- Good for screening and optimization

**Common Designs:**

- 2^(4-1)_IV: 8 runs, 4 factors
- 2^(5-1)_V: 16 runs, 5 factors
- 2^(7-3)_IV: 16 runs, 7 factors

## Sequential Experimentation

**Strategy:**

1. Screen with resolution III
2. Follow up with fold-over or augment
3. Progressively de-alias effects
4. Optimize with RSM

**Principle:** Learn as you go

## Resolution V Designs

**Properties:**

- Main effects clear of everything
- 2FI clear of each other
- Best for optimization
- Relatively large

**Common Designs:**

- 2^(5-1)_V: 16 runs, 5 factors
- 2^(6-1)_VI: 32 runs, 6 factors

---

# Additional Factorial Topics

## The 3^k Factorial Design

**Purpose:**

- Three levels per factor
- Can estimate quadratic effects
- More design points than 2^k

**Levels:**

- Low (0 or -)
- Medium (1 or 0)
- High (2 or +)

## Components in 3^k

**For each factor:**

- Linear component
- Quadratic component
- Total = 2 df per factor

**Interactions:**

- L×L, L×Q, Q×L, Q×Q components
- More complex than 2^k

## The 3² Design

**Nine Treatment Combinations:**

3² = 9 runs for 2 factors

**Effect Components:**

- A_linear, A_quadratic
- B_linear, B_quadratic
- AB_linear×linear, etc.

**Total:** 8 df for effects

## Analysis of 3^k Designs

**Sum of Squares:**

$$SS = \frac{(\text{contrast})^2}{n \times (\text{sum of squared coefficients})}$$

**ANOVA partitions each factor into L and Q**

## The General 3^k Design

**Characteristics:**

- 3^k treatment combinations
- 3^k - 1 degrees of freedom
- Gets large quickly
- Consider fractional designs

## Confounding in 3^k Designs

**Principle:** Same as 2^k

**Differences:**

- More complex modular arithmetic
- Two-way confounding schemes
- Latin squares for 3² in 3 blocks

## 3^k Fractional Factorials

**One-Third Fraction (3^(k-1)):**

- Run 1/3 of full factorial
- Use defining relation
- Create alias structure

**Example:** 3^(3-1) requires 9 runs instead of 27

## Mixed-Level Designs

**Motivation:**

- Some factors naturally have 2 levels
- Others have 3+ levels
- Don't force artificial structure

**Examples:**

- 2² × 3: 12 runs
- 2³ × 3²: 72 runs

## Factors at Two and Three Levels

**Design:** 2^m × 3^n

**Analysis:**

- Separate main effects for 2-level and 3-level
- Mixed interactions possible
- Use general linear model approach

## Factors at Two and Four Levels

**Option 1:** 2² × 2^k (treat 4 levels as 2×2)

**Option 2:** True 4-level factor

**Advantage of Option 1:**

- More structured interaction terms
- Easier interpretation

## Nonregular Fractional Factorials

**What are they?**

- Not from 2^(k-p) series
- Complex aliasing patterns
- No simple defining relation
- Better for certain situations

**When to use:**

- Need specific number of runs
- Want better properties than regular designs

## Plackett-Burman as Nonregular

**Properties:**

- 12, 20, 24 runs (not powers of 2)
- Orthogonal main effects
- Complex aliasing of interactions

**Advantage:** Efficient screening

## Nonregular Designs in 16 Runs

**Alternatives to 2^(7-3) and 2^(8-4):**

- Improved alias structures
- Better E(s²) efficiency
- More balanced confounding

**Construction:** Computer algorithms (optimal design)

## Analysis of Nonregular Designs

**Challenges:**

- Partial aliasing (not complete)
- No simple alias chains
- Need model selection methods

**Solutions:**

- Effect sparsity
- All subsets regression
- Stepwise methods
- Cross-validation

## Optimal Design

**Criteria:**

- **D-optimality:** Minimize |X'X|^(-1)
- **A-optimality:** Minimize trace(X'X)^(-1)
- **G-optimality:** Minimize maximum prediction variance
- **I-optimality:** Minimize average prediction variance

## D-Optimal Designs

**Most Common:**

- Minimizes volume of confidence region
- Maximizes determinant of X'X
- Good all-purpose criterion

**Use when:**

- Irregular design space
- Mixture of variable types
- Constrained regions

## Creating Optimal Designs

**Software Tools:**

- Design-Expert
- JMP
- R packages (AlgDesign)

**Process:**

1. Specify model
2. Choose criterion
3. Specify number of runs
4. Algorithm generates design
5. Evaluate and iterate

## Examples of Optimal Designs

**Applications:**

- Irregular regions (process constraints)
- Mixture experiments
- Split-plot with constraints
- Augmenting existing data

## Comparing Designs

**Metrics:**

- D-efficiency
- Prediction variance
- Condition number
- Power for effects

**Goal:** Balance between properties

## Summary: Design Selection

**Questions to ask:**

1. How many factors?
2. Primary objective (screening/optimization)?
3. How many runs feasible?
4. What resolution needed?
5. Any special constraints?

**Answer determines design choice**

---

# Summary and Conclusions

## Key Concepts from Chapters 1-9

**Foundation:**

- Design principles: Randomization, Replication, Blocking
- Statistical inference: Hypothesis testing, confidence intervals
- ANOVA: Partitioning variability

## Experimental Strategies

**Progression:**

1. **Screening:** Use fractional factorials (Resolution III-IV)
2. **Characterization:** Use full factorials or Resolution V
3. **Optimization:** Use response surface methods (Ch 11)

## Design Selection Guidelines

**By Number of Factors:**

- 2-4 factors: Full factorial
- 5-7 factors: Half fraction
- 8-15 factors: Highly fractionated or Plackett-Burman
- Many factors: Definitive screening designs

## Analysis Workflow

**Standard Process:**

1. Fit initial model
2. Check model adequacy (residual plots)
3. Transform if needed
4. Identify significant effects
5. Simplify model
6. Interpret and confirm

## Software Tools

**Recommended:**

- **R:** Full control, reproducible
- **JMP:** Interactive, visualization
- **Design-Expert:** Specialized for DOE
- **Minitab:** User-friendly, comprehensive

## Best Practices

**Planning:**

- Clear objectives
- Adequate replication
- Appropriate blocking
- Budget for follow-up

**Execution:**

- Randomize properly
- Document everything
- Check assumptions
- Validate conclusions

## Common Mistakes

**Avoid:**

- Changing one factor at a time
- Ignoring interactions
- Not checking assumptions
- Over-interpreting high-order interactions
- Insufficient replication

## Moving Forward

**Next Topics (Chapters 10-15):**

- Response surface methodology
- Robust parameter design
- Random and mixed models
- Nested and split-plot designs

## Final Thoughts

**Experimental design is:**

- Essential for efficient learning
- Applicable across all sciences
- Both art and science
- Continuous learning process

**Practice and experience are key!**

---